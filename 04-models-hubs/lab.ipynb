{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Week 4: Scale-Free Networks, Hubs & Resilience\n",
    "\n",
    "**Learning objectives** — After this lab you should be able to:\n",
    "\n",
    "- Explain why ER graphs cannot produce hubs (recap from Week 3)\n",
    "- Describe preferential attachment and its role in hub formation\n",
    "- Compare degree distributions of synthetic models to real networks\n",
    "- Visualize power laws using CCDF and estimate exponents with MLE\n",
    "- Explain the Molloy-Reed criterion for giant component existence\n",
    "- Demonstrate the robustness paradox: robust to random failure, fragile to targeted attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "Why do some airports have 200 routes while most have only 5? Why do a few Twitter accounts\n",
    "have millions of followers? Last week we saw that real networks are small worlds and explored\n",
    "the ER random model as a baseline. This week we discover that only one model can explain the\n",
    "extreme **hubs** we see in real data — and we'll see why those hubs make networks both\n",
    "powerful and vulnerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.stats import poisson\n",
    "from netsci.loaders import load_graph\n",
    "from netsci.utils import SEED, graph_summary, fit_power_law\n",
    "from netsci import viz, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec1-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec1-datasets-context",
   "metadata": {},
   "source": [
    "We revisit three familiar networks this week:\n",
    "\n",
    "- **Facebook** (334 nodes) — a dense social graph with a fat-tailed degree distribution and high clustering. Its hubs and tight communities make it a good test case for the BA model.\n",
    "- **US Power Grid** (4,941 nodes) — a sparse infrastructure network with narrow degree distribution and low clustering. Its engineering constraints make it a challenge for all models.\n",
    "- **US Airports** (500 nodes, ~2,980 edges) — a hub-and-spoke topology where a few major airports connect to hundreds of others. We’ll use this later to study how hubs affect network resilience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_fb = load_graph(\"facebook\")\n",
    "graph_summary(G_fb)\n",
    "print()\n",
    "G_pg = load_graph(\"powergrid\")\n",
    "graph_summary(G_pg)\n",
    "print()\n",
    "G_air = load_graph(\"airports\")\n",
    "graph_summary(G_air)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec2-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Hub Puzzle\n",
    "\n",
    "Let's look at the Facebook degree distribution on a log-log scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fb-loglog",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_degree_dist(G_fb, log=True, title=\"Facebook — degree distribution (log-log)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-hub-explain",
   "metadata": {},
   "source": [
    "The **fat tail** on the right shows that a few nodes have enormous degree — these are **hubs**.\n",
    "Most nodes have low degree, but the hubs dominate the network's structure.\n",
    "\n",
    "Can our models explain this? Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec3-recap-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ER & WS Recap\n",
    "\n",
    "Last week we explored the **Erdos-Renyi** and **Watts-Strogatz** models in detail.\n",
    "Let's quickly generate both and ask: can either explain the hubs we see in Facebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-recap-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick recap: generate ER and WS models, compare to Facebook\n",
    "G_er = models.erdos_renyi(n=500, avg_degree=6)\n",
    "G_ws = models.watts_strogatz(n=500, k=6, p=0.1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "for ax, (name, G) in zip(\n",
    "    axes, [(\"Erdos-Renyi\", G_er), (\"Watts-Strogatz\", G_ws), (\"Facebook\", G_fb)]\n",
    "):\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "    deg_count = Counter(degrees)\n",
    "    x, y = zip(*sorted(deg_count.items()))\n",
    "    ax.scatter(x, y, s=30, alpha=0.7, edgecolors=\"white\", linewidth=0.5)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Degree (k)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"{name} (log-log)\")\n",
    "fig.suptitle(\"Can ER or WS explain hubs?\", fontsize=13, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats comparison\n",
    "print(f\"{'Model':20s} {'Max degree':>10s} {'Clustering':>10s}\")\n",
    "print(\"-\" * 42)\n",
    "for name, G in [(\"ER (n=500)\", G_er), (\"WS (n=500)\", G_ws), (\"Facebook\", G_fb)]:\n",
    "    max_d = max(d for _, d in G.degree())\n",
    "    C = nx.average_clustering(G)\n",
    "    print(f\"{name:20s} {max_d:10d} {C:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-recap-conclusion",
   "metadata": {},
   "source": [
    "**Neither model explains hubs.** ER produces a narrow Poisson degree distribution — no fat tail. WS starts from a regular lattice and rewiring barely changes the degree distribution. Both models produce max degrees around 10-15, while Facebook has nodes with 100+ connections. We need a fundamentally different mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec5-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Barabasi-Albert Model\n",
    "\n",
    "The key idea is **preferential attachment** — \"the rich get richer.\"\n",
    "\n",
    "New nodes join the network one at a time. Each new node connects to *m* existing nodes,\n",
    "but prefers to connect to nodes that **already have many connections**.\n",
    "This naturally produces a **power-law** degree distribution with hubs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ba-two-rules",
   "metadata": {},
   "source": [
    "The BA model rests on two simple rules:\n",
    "\n",
    "1. **Growth** — the network starts small and new nodes arrive one at a time (unlike ER/WS which start with all nodes present).\n",
    "2. **Preferential attachment** — each new node connects to *m* existing nodes, but prefers high-degree nodes. The probability of connecting to node *i* is proportional to its current degree: P(i) = k_i / Σk.\n",
    "\n",
    "The result: early nodes accumulate connections over time (“the rich get richer”), producing the extreme hubs we see in real networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ba-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_ba = models.barabasi_albert(n=500, m=3)\n",
    "graph_summary(G_ba)\n",
    "print(f\"Max degree: {max(d for _, d in G_ba.degree())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ba-degdist",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_degree_dist(G_ba, log=True, title=\"Barabasi-Albert (log-log) — power law!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ba-degdist-interp",
   "metadata": {},
   "source": [
    "**The straight line**: On a log-log scale, the BA degree distribution approximates a straight line — the hallmark of a **power law** P(k) ~ k^(-γ) with γ ≈ 3. The maximum degree (typically 30-80 for n=500) is far larger than ER’s maximum (~15-20). These extreme hubs emerge naturally from preferential attachment without any explicit “hub creation” rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ajp7ih8p8l",
   "metadata": {},
   "source": [
    "**Try it yourself**: Generate two BA graphs with m=1 and m=5 (both n=500). Which has a higher max degree? Why does this make sense given the preferential attachment mechanism?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a581pclsfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "G_ba_m1 = models.barabasi_albert(n=500, m=1)\n",
    "G_ba_m5 = models.barabasi_albert(n=500, m=5)\n",
    "\n",
    "max_deg_m1 = max(d for _, d in G_ba_m1.degree())\n",
    "max_deg_m5 = max(d for _, d in G_ba_m5.degree())\n",
    "assert max_deg_m5 > max_deg_m1, (\n",
    "    \"Hint: m=5 should produce a higher max degree — each new node adds more edges\"\n",
    ")\n",
    "print(\n",
    "    f\"BA(m=1): max degree = {max_deg_m1}, avg degree = {2 * G_ba_m1.number_of_edges() / 500:.1f}\"\n",
    ")\n",
    "print(\n",
    "    f\"BA(m=5): max degree = {max_deg_m5}, avg degree = {2 * G_ba_m5.number_of_edges() / 500:.1f}\"\n",
    ")\n",
    "print(f\"m=5 produces larger hubs because more edges feed the rich-get-richer process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ba-explain",
   "metadata": {},
   "source": [
    "On the log-log plot, BA degree distribution approximates a straight line — the hallmark of a **power law**. Hubs emerge naturally from the preferential attachment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ba-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BA growth snapshots: same graph at n=5, n=10, n=20\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "snapshots = [5, 10, 20]\n",
    "\n",
    "# Build a BA graph incrementally up to max snapshot size\n",
    "G_grow = nx.barabasi_albert_graph(max(snapshots), 2, seed=SEED)\n",
    "\n",
    "for ax, n_snap in zip(axes, snapshots):\n",
    "    G_sub = G_grow.subgraph(range(n_snap)).copy()\n",
    "    pos = nx.spring_layout(G_sub, seed=SEED)\n",
    "    degrees = dict(G_sub.degree())\n",
    "    sizes = [degrees[n] * 80 + 40 for n in G_sub.nodes()]\n",
    "    nx.draw_networkx(\n",
    "        G_sub,\n",
    "        pos,\n",
    "        ax=ax,\n",
    "        node_color=\"#4878CF\",\n",
    "        node_size=sizes,\n",
    "        edge_color=\"#cccccc\",\n",
    "        width=1.0,\n",
    "        with_labels=True,\n",
    "        font_size=8,\n",
    "        font_color=\"white\",\n",
    "    )\n",
    "    ax.set_title(f\"n = {n_snap}\\nmax degree = {max(degrees.values())}\", fontsize=11)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"BA Growth: node size ∙ degree (early nodes become hubs)\", fontsize=13)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec6-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Side-by-Side Comparison\n",
    "\n",
    "This is the centerpiece figure: three models compared across degree distribution, graph structure, and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec6-predict",
   "metadata": {},
   "source": [
    "**Before you look**: Based on what you know about each model, predict:\n",
    "- Which model will show a **straight line** on the log-log degree plot? (Hint: power law)\n",
    "- Which will have the **highest clustering**? (Hint: which starts from a lattice?)\n",
    "- Which will have the **largest max degree**? (Hint: rich-get-richer)\n",
    "\n",
    "Check your predictions against the 3×3 panel below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3x3-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "\n",
    "model_graphs = [\n",
    "    (\"Erdos-Renyi\", G_er),\n",
    "    (\"Watts-Strogatz\", G_ws),\n",
    "    (\"Barabasi-Albert\", G_ba),\n",
    "]\n",
    "\n",
    "for row, (name, G) in enumerate(model_graphs):\n",
    "    # Column 0: Degree distribution (log-log)\n",
    "    ax = axes[row, 0]\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "    deg_count = Counter(degrees)\n",
    "    x, y = zip(*sorted(deg_count.items()))\n",
    "    ax.scatter(x, y, s=20, alpha=0.7, edgecolors=\"white\", linewidth=0.3)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Degree (k)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"{name}\\nDegree dist (log-log)\")\n",
    "\n",
    "    # Column 1: Graph drawing (subset of 100 nodes for readability)\n",
    "    ax = axes[row, 1]\n",
    "    nodes_sub = list(G.nodes())[:100]\n",
    "    G_sub = G.subgraph(nodes_sub)\n",
    "    pos = nx.spring_layout(G_sub, seed=SEED)\n",
    "    nx.draw_networkx(\n",
    "        G_sub,\n",
    "        pos,\n",
    "        ax=ax,\n",
    "        node_color=\"#4878CF\",\n",
    "        node_size=20,\n",
    "        edge_color=\"#cccccc\",\n",
    "        width=0.3,\n",
    "        with_labels=False,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    ax.set_title(f\"{name}\\nGraph (first 100 nodes)\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Column 2: Statistics\n",
    "    ax = axes[row, 2]\n",
    "    ax.axis(\"off\")\n",
    "    stats_text = (\n",
    "        f\"Nodes: {G.number_of_nodes()}\\n\"\n",
    "        f\"Edges: {G.number_of_edges()}\\n\"\n",
    "        f\"Avg degree: {np.mean(degrees):.1f}\\n\"\n",
    "        f\"Max degree: {max(degrees)}\\n\"\n",
    "        f\"Clustering: {nx.average_clustering(G):.4f}\\n\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.1,\n",
    "        0.5,\n",
    "        stats_text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=13,\n",
    "        verticalalignment=\"center\",\n",
    "        fontfamily=\"monospace\",\n",
    "    )\n",
    "    ax.set_title(f\"{name}\\nStatistics\")\n",
    "\n",
    "fig.suptitle(\"Three Network Models Compared\", fontsize=16, y=1.01)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3x3-interp",
   "metadata": {},
   "source": [
    "**Reading the statistics column**: ER has low clustering (~0.02) and moderate max degree. WS has the highest clustering (~0.4) but a narrow max degree. BA has the highest max degree but low clustering (~0.01). No single model captures both hubs *and* clustering simultaneously — this is a fundamental limitation that drives research into more advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec7-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model vs Reality\n",
    "\n",
    "How well does BA match the Facebook network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-vs-real",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a BA graph with same n as Facebook\n",
    "n_fb = G_fb.number_of_nodes()\n",
    "avg_deg_fb = 2 * G_fb.number_of_edges() / n_fb\n",
    "m_ba = max(1, round(avg_deg_fb / 2))  # BA avg_deg ≈ 2m\n",
    "G_ba_fb = models.barabasi_albert(n_fb, m_ba)\n",
    "\n",
    "# Overlay degree distributions\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "for G, label, marker in [\n",
    "    (G_fb, \"Facebook (real)\", \"o\"),\n",
    "    (G_ba_fb, f\"BA (n={n_fb}, m={m_ba})\", \"s\"),\n",
    "]:\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "    deg_count = Counter(degrees)\n",
    "    x, y = zip(*sorted(deg_count.items()))\n",
    "    ax.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        s=30,\n",
    "        alpha=0.7,\n",
    "        marker=marker,\n",
    "        label=label,\n",
    "        edgecolors=\"white\",\n",
    "        linewidth=0.3,\n",
    "    )\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Degree (k)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Facebook vs BA Model\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare stats\n",
    "print(f\"{'':20s} {'Facebook':>10s} {'BA model':>10s}\")\n",
    "print(\n",
    "    f\"{'Avg clustering':20s} {nx.average_clustering(G_fb):10.4f} {nx.average_clustering(G_ba_fb):10.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Max degree':20s} {max(d for _, d in G_fb.degree()):10d} {max(d for _, d in G_ba_fb.degree()):10d}\"\n",
    ")\n",
    "print(f\"\\nBA captures hubs but misses the high clustering of real social networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ba-gap",
   "metadata": {},
   "source": [
    "**The gap**: BA captures the fat tail (hubs) remarkably well, but its clustering coefficient is an order of magnitude lower than Facebook’s. Real social networks have both hubs *and* tight friend-groups — BA explains the former but not the latter. Models that combine preferential attachment with local clustering (like the Holme-Kim model) can close this gap, but no single simple model captures everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec8-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Tweak & Observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec8-predict",
   "metadata": {},
   "source": [
    "**Predict before you tweak**: Increasing *m* (edges per new node) in BA means each newcomer makes more connections. Will this increase or decrease the maximum hub degree? Will it change the *shape* of the distribution or just shift it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tweak-ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TWEAK: Change m in BA, observe how hub size changes ----\n",
    "m_tweak = 3  # <-- change me (try 1, 3, 5, 10)\n",
    "\n",
    "G_ba_tw = models.barabasi_albert(n=500, m=m_tweak)\n",
    "max_deg = max(d for _, d in G_ba_tw.degree())\n",
    "print(f\"BA(n=500, m={m_tweak}): max degree = {max_deg}\")\n",
    "viz.plot_degree_dist(G_ba_tw, log=True, title=f\"BA (m={m_tweak}) — log-log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-tweak-ba-interp",
   "metadata": {},
   "source": [
    "**What you should see**: Increasing *m* raises the average degree and shifts the entire distribution rightward, but the **power-law slope stays roughly the same** (γ ≈ 3). The maximum hub degree increases because more connections are available overall, but the relative inequality between hubs and leaves persists. The shape is governed by the preferential attachment mechanism, not by *m* alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec9-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. The Right Way to Visualize Power Laws\n",
    "\n",
    "The log-log scatter plots we've been using have a problem: **noisy tails**. At high degree values, there are very few nodes, so the points scatter wildly. Binning into histograms introduces artifacts (the result depends heavily on bin width).\n",
    "\n",
    "The solution: the **Complementary CDF (CCDF)**, which plots P(K ≥ k) — the probability that a randomly chosen node has degree *at least* k. On log-log axes:\n",
    "- A **power law** → straight line with slope -(γ-1)\n",
    "- An **exponential** → downward-curving line\n",
    "- A **Poisson** → steep drop-off\n",
    "\n",
    "The CCDF is cumulative and requires no binning, giving a much cleaner signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ccdf-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCDF comparison: Facebook vs BA vs ER\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "graphs = [\n",
    "    (\"Facebook (real)\", G_fb),\n",
    "    (\"BA model (n=500)\", G_ba),\n",
    "    (\"ER model (n=500)\", G_er),\n",
    "]\n",
    "\n",
    "for ax, (name, G) in zip(axes, graphs):\n",
    "    degrees = sorted([d for _, d in G.degree() if d > 0], reverse=True)\n",
    "    n = len(degrees)\n",
    "    ccdf_y = np.arange(1, n + 1) / n\n",
    "    ax.scatter(degrees, ccdf_y, s=15, alpha=0.7, edgecolors=\"white\", linewidth=0.3)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Degree (k)\")\n",
    "    ax.set_ylabel(\"P(K ≥ k)\")\n",
    "    ax.set_title(name)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"CCDF: Power Law → Straight Line, Exponential → Curved\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ccdf-interp",
   "metadata": {},
   "source": [
    "**Reading the CCDF**: Facebook and BA both show approximately straight lines on the CCDF — the hallmark of a power-law distribution. ER curves downward sharply, confirming its exponential (Poisson) tail. The CCDF gives a much cleaner picture than the raw degree count scatter because every data point contributes to the curve without binning artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ccdf-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the viz helper for a quick CCDF with fit line\n",
    "viz.plot_ccdf(G_fb, title=\"Facebook CCDF with MLE fit\", fit_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec10-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Power-Law Fitting (MLE)\n",
    "\n",
    "How do we estimate the exponent γ of a power law P(k) ~ k^(-γ)?\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)** gives the best estimate:\n",
    "\n",
    "$$\\hat{\\alpha} = 1 + n \\left[ \\sum_{i=1}^{n} \\ln \\frac{x_i}{x_{\\min}} \\right]^{-1}$$\n",
    "\n",
    "where x_i are the observed degree values ≥ x_min, and n is the count of such values.\n",
    "\n",
    "The intuition: MLE finds the exponent that makes the observed data most probable under the power-law model. Unlike fitting a line to a log-log plot (which gives biased estimates), MLE is statistically principled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-mle-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to Facebook, airports, and BA model\n",
    "networks = [\n",
    "    (\"Facebook\", [d for _, d in G_fb.degree()]),\n",
    "    (\"Airports\", [d for _, d in G_air.degree()]),\n",
    "    (\"BA model\", [d for _, d in G_ba.degree()]),\n",
    "]\n",
    "\n",
    "for name, degs in networks:\n",
    "    alpha = fit_power_law(degs, k_min=2)\n",
    "    print(f\"{name:12s}: α = {alpha:.2f}  (γ = α ≈ {alpha:.1f})\")\n",
    "\n",
    "print(f\"\\nNote: BA theory predicts γ = 3.0 exactly.\")\n",
    "print(f\"Real networks typically have γ between 2 and 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-mle-ccdf-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MLE fit line on CCDF for all three\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "for ax, (name, degs) in zip(axes, networks):\n",
    "    degs_sorted = sorted([d for d in degs if d > 0], reverse=True)\n",
    "    n = len(degs_sorted)\n",
    "    ccdf_y = np.arange(1, n + 1) / n\n",
    "    ax.scatter(degs_sorted, ccdf_y, s=15, alpha=0.6, edgecolors=\"white\", linewidth=0.3)\n",
    "\n",
    "    # MLE fit line\n",
    "    k_min = 2\n",
    "    alpha = fit_power_law(degs, k_min=k_min)\n",
    "    k_range = np.logspace(np.log10(k_min), np.log10(max(degs)), 200)\n",
    "    fit_y = (k_range / k_min) ** (-(alpha - 1))\n",
    "    ax.plot(k_range, fit_y, \"r--\", lw=1.5, label=f\"α = {alpha:.2f}\")\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Degree (k)\")\n",
    "    ax.set_ylabel(\"P(K ≥ k)\")\n",
    "    ax.set_title(name)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"CCDF with MLE Power-Law Fit\", fontsize=13, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-mle-interp",
   "metadata": {},
   "source": [
    "**Interpreting the exponents**: The BA model's α ≈ 3.0 matches the theoretical prediction exactly — this is a strong validation of the MLE method. Real networks (Facebook, airports) deviate from γ = 3 because they involve more complex mechanisms than pure preferential attachment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec11-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Is It Really a Power Law?\n",
    "\n",
    "Many networks *look* like power laws on a log-log plot, but are they really? Let's compare the Facebook degree distribution against three theoretical distributions:\n",
    "\n",
    "1. **Power law**: P(k) ~ k^(-γ) — straight line on CCDF\n",
    "2. **Exponential**: P(k) ~ e^(-k/λ) — the ER/Poisson tail\n",
    "3. **Log-normal**: P(k) ~ (1/k) · e^(-(ln k - μ)²/2σ²) — often confused with power laws\n",
    "\n",
    "The key message: **proper methodology matters**. Clauset, Shalizi & Newman (2009) showed that many \"scale-free\" claims in the literature don't survive rigorous statistical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dist-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Facebook CCDF to theoretical distributions\n",
    "fb_degs = sorted([d for _, d in G_fb.degree() if d > 0], reverse=True)\n",
    "n = len(fb_degs)\n",
    "ccdf_y = np.arange(1, n + 1) / n\n",
    "\n",
    "# Theoretical CCDFs\n",
    "k_range = np.logspace(np.log10(1), np.log10(max(fb_degs)), 300)\n",
    "\n",
    "# Power law fit\n",
    "alpha_fb = fit_power_law([d for _, d in G_fb.degree()], k_min=2)\n",
    "ccdf_pl = (k_range / 2) ** (-(alpha_fb - 1))\n",
    "ccdf_pl = ccdf_pl / ccdf_pl[0]  # normalize\n",
    "\n",
    "# Exponential fit (match mean)\n",
    "mean_deg = np.mean([d for _, d in G_fb.degree()])\n",
    "ccdf_exp = np.exp(-k_range / mean_deg)\n",
    "ccdf_exp = ccdf_exp / ccdf_exp[0]\n",
    "\n",
    "# Poisson (what ER produces)\n",
    "ccdf_poisson = 1 - poisson.cdf(k_range, mean_deg)\n",
    "ccdf_poisson = np.maximum(ccdf_poisson, 1e-10)  # avoid log(0)\n",
    "\n",
    "with plt.style.context(\"seaborn-v0_8-muted\"):\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    ax.scatter(\n",
    "        fb_degs,\n",
    "        ccdf_y,\n",
    "        s=15,\n",
    "        alpha=0.5,\n",
    "        label=\"Facebook (data)\",\n",
    "        zorder=5,\n",
    "        edgecolors=\"white\",\n",
    "        linewidth=0.3,\n",
    "    )\n",
    "    ax.plot(k_range, ccdf_pl, \"r--\", lw=1.5, label=f\"Power law (\\u03b1={alpha_fb:.2f})\")\n",
    "    ax.plot(k_range, ccdf_exp, \"g-.\", lw=1.5, label=\"Exponential\")\n",
    "    ax.plot(k_range, ccdf_poisson, \"m:\", lw=1.5, label=f\"Poisson (\\u03bb={mean_deg:.1f})\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Degree (k)\")\n",
    "    ax.set_ylabel(\"P(K \\u2265 k)\")\n",
    "    ax.set_title(\"Facebook: Data vs Theoretical Distributions\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_ylim(bottom=1e-3)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-dist-compare-interp",
   "metadata": {},
   "source": [
    "**What the comparison shows**: The power-law fit tracks the data's tail much better than the exponential or Poisson. However, the fit isn't perfect — the data deviates at both low and high degrees. This is typical: real degree distributions are often *approximately* power-law in the tail but not across the full range. The debate over whether networks are \"truly\" scale-free remains active in the literature (Broido & Clauset, 2019).\n",
    "\n",
    "**Takeaway**: Always use CCDF + MLE when claiming power-law behavior. Never rely on \"it looks straight on a log-log plot.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sec11-robustness-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Network Robustness\n",
    "\n",
    "Hubs are powerful — but they're also Achilles' heels. What happens when they fail?\n",
    "\n",
    "**Key concept**: removing nodes = site percolation. The central question is: at what fraction of removed nodes does the **giant component** vanish?\n",
    "\n",
    "**The Molloy-Reed criterion** gives a condition for the giant component to exist:\n",
    "\n",
    "$$\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} > 2$$\n",
    "\n",
    "where ⟨k⟩ is the average degree and ⟨k²⟩ is the average squared degree. When this ratio drops below 2, the giant component disintegrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-molloy-reed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Molloy-Reed criterion for our networks\n",
    "for name, G in [(\"Airports\", G_air), (\"Facebook\", G_fb), (\"Power Grid\", G_pg)]:\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "    k_avg = np.mean(degrees)\n",
    "    k2_avg = np.mean(np.array(degrees) ** 2)\n",
    "    kappa = k2_avg / k_avg\n",
    "    print(f\"{name:12s}: ⟨k⟩ = {k_avg:.1f},  ⟨k²⟩/⟨k⟩ = {kappa:.1f}  (threshold = 2)\")\n",
    "    print(f\"{'':<12s}  Giant component {'EXISTS' if kappa > 2 else 'DOES NOT EXIST'}\")\n",
    "    print()\n",
    "\n",
    "print(\"The higher ⟨k²⟩/⟨k⟩ is above 2, the more resilient the network.\")\n",
    "print(\n",
    "    \"Fat-tailed networks (airports) have very high ⟨k²⟩, making them robust to random failure.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-robustness-predict",
   "metadata": {},
   "source": [
    "**Predict before you run**: If we remove nodes randomly vs removing the highest-degree nodes first, which strategy will destroy the giant component faster? Think about what happens to ⟨k²⟩/⟨k⟩ in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-robustness-sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random vs targeted attack sweep — track giant component size\n",
    "fractions = np.arange(0, 0.51, 0.02)\n",
    "rng_robust = np.random.default_rng(SEED)\n",
    "\n",
    "results_robust = {}\n",
    "for name, G in [(\"Airports\", G_air), (\"Facebook\", G_fb)]:\n",
    "    gcc_random = []\n",
    "    gcc_targeted = []\n",
    "    nodes = list(G.nodes())\n",
    "    N = len(nodes)\n",
    "\n",
    "    for f in fractions:\n",
    "        n_remove = int(N * f)\n",
    "\n",
    "        # Random removal\n",
    "        G_rand = G.copy()\n",
    "        remove_rand = set(rng_robust.choice(nodes, size=n_remove, replace=False))\n",
    "        G_rand.remove_nodes_from(remove_rand)\n",
    "        if G_rand.number_of_nodes() > 0:\n",
    "            gcc_rand = len(max(nx.connected_components(G_rand), key=len)) / N\n",
    "        else:\n",
    "            gcc_rand = 0\n",
    "        gcc_random.append(gcc_rand)\n",
    "\n",
    "        # Targeted removal (highest degree first, recalculated)\n",
    "        G_targ = G.copy()\n",
    "        for _ in range(n_remove):\n",
    "            if G_targ.number_of_nodes() == 0:\n",
    "                break\n",
    "            top_node = max(G_targ.nodes(), key=lambda n: G_targ.degree(n))\n",
    "            G_targ.remove_node(top_node)\n",
    "        if G_targ.number_of_nodes() > 0:\n",
    "            gcc_targ = len(max(nx.connected_components(G_targ), key=len)) / N\n",
    "        else:\n",
    "            gcc_targ = 0\n",
    "        gcc_targeted.append(gcc_targ)\n",
    "\n",
    "    results_robust[name] = (gcc_random, gcc_targeted)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for ax, name in zip(axes, [\"Airports\", \"Facebook\"]):\n",
    "    gcc_rand, gcc_targ = results_robust[name]\n",
    "    ax.plot(fractions * 100, gcc_rand, \"o-\", label=\"Random failure\", markersize=4)\n",
    "    ax.plot(fractions * 100, gcc_targ, \"s-\", label=\"Targeted attack\", markersize=4)\n",
    "    ax.set_xlabel(\"% of nodes removed\")\n",
    "    ax.set_ylabel(\"Giant component (fraction of N)\")\n",
    "    ax.set_title(f\"{name}\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\":\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Network Robustness: Random Failure vs Targeted Attack\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-robustness-interpret",
   "metadata": {},
   "source": [
    "**The robustness paradox**: Scale-free networks (like airports) are remarkably **robust to random failure** — you can remove 30-40% of nodes randomly and the giant component barely shrinks. But they are **fragile to targeted attack** — removing just 10-15% of the highest-degree hubs shatters the network.\n",
    "\n",
    "**The mathematical reason**: Random removal barely changes ⟨k²⟩/⟨k⟩ because most removed nodes are low-degree (the vast majority in a power-law distribution). But removing hubs destroys the high-k² terms that keep the ratio above 2.\n",
    "\n",
    "This paradox has profound implications: the same hub structure that makes scale-free networks efficient also makes them vulnerable to deliberate attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-robustness-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immunization concept: random removal vs targeted hub removal\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "pos_fb = nx.spring_layout(G_fb, seed=SEED)\n",
    "rng_imm = np.random.default_rng(SEED)\n",
    "frac_remove = 0.10\n",
    "n_remove = int(G_fb.number_of_nodes() * frac_remove)\n",
    "\n",
    "# Panel 1: Random removal\n",
    "G_rand_rm = G_fb.copy()\n",
    "rand_remove = set(rng_imm.choice(list(G_fb.nodes()), size=n_remove, replace=False))\n",
    "G_rand_rm.remove_nodes_from(rand_remove)\n",
    "comps_rand = sorted(nx.connected_components(G_rand_rm), key=len, reverse=True)\n",
    "colors_rand = []\n",
    "for n in G_fb.nodes():\n",
    "    if n in rand_remove:\n",
    "        colors_rand.append(\"#FFFFFF\")\n",
    "    elif n in comps_rand[0]:\n",
    "        colors_rand.append(\"#4878CF\")\n",
    "    else:\n",
    "        colors_rand.append(\"#CCCCCC\")\n",
    "sizes_rand = [0 if n in rand_remove else 30 for n in G_fb.nodes()]\n",
    "nx.draw_networkx_nodes(\n",
    "    G_fb, pos_fb, ax=axes[0], node_color=colors_rand, node_size=sizes_rand\n",
    ")\n",
    "edges_rand = [\n",
    "    (u, v) for u, v in G_fb.edges() if u not in rand_remove and v not in rand_remove\n",
    "]\n",
    "nx.draw_networkx_edges(\n",
    "    G_fb, pos_fb, ax=axes[0], edgelist=edges_rand, edge_color=\"#cccccc\", width=0.3\n",
    ")\n",
    "gcc_rand = len(comps_rand[0]) / G_fb.number_of_nodes()\n",
    "axes[0].set_title(\n",
    "    f\"Random removal ({frac_remove:.0%} of nodes)\\nLargest component: {gcc_rand:.0%} of network\",\n",
    "    fontsize=11,\n",
    ")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Panel 2: Targeted removal (top hubs)\n",
    "G_targ_rm = G_fb.copy()\n",
    "hubs_sorted = sorted(G_fb.nodes(), key=lambda n: G_fb.degree(n), reverse=True)\n",
    "targ_remove = set(hubs_sorted[:n_remove])\n",
    "G_targ_rm.remove_nodes_from(targ_remove)\n",
    "comps_targ = sorted(nx.connected_components(G_targ_rm), key=len, reverse=True)\n",
    "colors_targ = []\n",
    "for n in G_fb.nodes():\n",
    "    if n in targ_remove:\n",
    "        colors_targ.append(\"#FFFFFF\")\n",
    "    elif n in comps_targ[0]:\n",
    "        colors_targ.append(\"#D65F5F\")\n",
    "    else:\n",
    "        colors_targ.append(\"#CCCCCC\")\n",
    "sizes_targ = [0 if n in targ_remove else 30 for n in G_fb.nodes()]\n",
    "nx.draw_networkx_nodes(\n",
    "    G_fb, pos_fb, ax=axes[1], node_color=colors_targ, node_size=sizes_targ\n",
    ")\n",
    "edges_targ = [\n",
    "    (u, v) for u, v in G_fb.edges() if u not in targ_remove and v not in targ_remove\n",
    "]\n",
    "nx.draw_networkx_edges(\n",
    "    G_fb, pos_fb, ax=axes[1], edgelist=edges_targ, edge_color=\"#cccccc\", width=0.3\n",
    ")\n",
    "gcc_targ = len(comps_targ[0]) / G_fb.number_of_nodes()\n",
    "axes[1].set_title(\n",
    "    f\"Targeted hub removal ({frac_remove:.0%} of nodes)\\nLargest component: {gcc_targ:.0%} of network\",\n",
    "    fontsize=11,\n",
    ")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Robustness Paradox: Random vs Targeted Removal\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-robustness-viz-interp",
   "metadata": {},
   "source": [
    "**The hub removal effect**: Removing 10% of nodes at random (left) barely dents the network — the giant component remains mostly intact. But removing the top 10% highest-degree hubs (right) shatters it into disconnected fragments. This asymmetry is why targeted attacks on infrastructure networks (power grids, internet backbone) are so dangerous — and why understanding hubs matters for network defense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Model | Degree dist | Clustering | Hubs? | Key mechanism |\n",
    "|-------|------------|-----------|-------|---------------|\n",
    "| **Erdos-Renyi** | Poisson (narrow) | Low | No | Random edges |\n",
    "| **Watts-Strogatz** | Narrow (near-uniform) | High | No | Rewired lattice |\n",
    "| **Barabasi-Albert** | Power law (fat tail) | Low | Yes | Preferential attachment |\n",
    "\n",
    "### Analysis Methods\n",
    "\n",
    "| Method | Purpose | Key insight |\n",
    "|--------|---------|-------------|\n",
    "| **CCDF plot** | Visualize degree distributions cleanly | No binning artifacts; power law = straight line |\n",
    "| **MLE fitting** | Estimate power-law exponent | Statistically principled; avoids log-log regression bias |\n",
    "| **Distribution comparison** | Test \"is it really a power law?\" | Compare power law vs exponential vs Poisson |\n",
    "| **Molloy-Reed criterion** | Test giant component existence | ⟨k²⟩/⟨k⟩ > 2 for connected network |\n",
    "| **Robustness paradox** | Targeted vs random failure | Hubs = robust to random, fragile to targeted |\n",
    "\n",
    "No single model captures everything about real networks. BA explains hubs but misses clustering.\n",
    "WS explains clustering but misses hubs. And the hubs that make networks efficient also make them vulnerable to targeted attack.\n",
    "\n",
    "Next week: **Community Detection** — finding groups within networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
